{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "\n",
    "# Load the images in grayscale\n",
    "image1 = cv2.imread('image1.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "image2 = cv2.imread('image2.jpg', cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Compute SSIM between the two images\n",
    "ssim_index, ssim_map = ssim(image1, image2, full=True)\n",
    "\n",
    "print(\"SSIM:\", ssim_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "\n",
    "# Example feature maps\n",
    "feature_map1 = np.random.rand(64, 64)  # Example feature map\n",
    "feature_map2 = np.random.rand(64, 64)  # Another example feature map\n",
    "\n",
    "# Compute SSIM\n",
    "ssim_index, ssim_map = ssim(feature_map1, feature_map2, full=True)\n",
    "\n",
    "print(\"SSIM between feature maps:\", ssim_index)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "# Example feature vectors\n",
    "feature_vector1 = np.random.rand(128)  # 128-dimensional feature vector\n",
    "feature_vector2 = np.random.rand(128)\n",
    "\n",
    "# Calculate cosine similarity\n",
    "cosine_sim = cosine_similarity([feature_vector1], [feature_vector2])\n",
    "\n",
    "print(\"Cosine Similarity:\", cosine_sim[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ORB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def orb_feature_compare(imageA, imageB):\n",
    "    # Convert to grayscale\n",
    "    imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "    imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Find the keypoints and descriptors with ORB\n",
    "    kpA, desA = orb.detectAndCompute(imageA, None)\n",
    "    kpB, desB = orb.detectAndCompute(imageB, None)\n",
    "\n",
    "    # Match descriptors\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(desA, desB)\n",
    "    \n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    return len(matches), matches\n",
    "\n",
    "# Load images\n",
    "imageA = cv2.imread('image1.webp')\n",
    "imageB = cv2.imread('image2.webp')\n",
    "\n",
    "# Compare features using ORB\n",
    "matches_count, matches = orb_feature_compare(imageA, imageB)\n",
    "print(f\"Number of Matches: {matches_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad-CAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cifar10\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Start Loading noises\n",
      "Creating noises loader\n",
      "Selecting flip as positive pair for class 0\n",
      "Selecting glass_blur as negetive pair for class 0\n",
      "Loading noises finished!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from models.resnet import ResNet18\n",
    "from torchvision.transforms import transforms\n",
    "from dataset_loader import noise_loader, load_cifar10, load_svhn, load_cifar100, load_imagenet, load_mvtec_ad, load_visa\n",
    "import json\n",
    "import argparse\n",
    "\n",
    "\n",
    "from clip import clip\n",
    "from torchvision.models import wide_resnet50_2\n",
    "\n",
    "def prep_output(img_tensor):\n",
    "    invTrans = torchvision.transforms.Compose([ torchvision.transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                    std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                            torchvision.transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                    std = [ 1., 1., 1. ]),\n",
    "                            ])\n",
    "    out = invTrans(img_tensor)\n",
    "    out = out.detach().numpy().transpose(1, 2, 0)\n",
    "\n",
    "    return out\n",
    "\n",
    "def plot_maps(img1, img2,vmin=0.3,vmax=0.7, mix_val=2):\n",
    "    f = plt.figure(figsize=(10,30))\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(img1,vmin=vmin, vmax=vmax, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(img2, cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(img1*mix_val+img2/mix_val, cmap=\"gray\" )\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "def norm_flat_image(img):\n",
    "    grads_norm = prep_output(img)\n",
    "    grads_norm = grads_norm[:,:,0]+ grads_norm[:,:,1]+ grads_norm[:,:,2]\n",
    "    grads_norm = (grads_norm - np.min(grads_norm))/ (np.max(grads_norm)- np.min(grads_norm))\n",
    "    return grads_norm\n",
    "\n",
    "\n",
    "def loading_datasets(args, data_path, imagenet_path):\n",
    "    if args.dataset == 'cifar10':\n",
    "        args.num_classes = 10\n",
    "        train_loader, test_loader = load_cifar10(data_path, \n",
    "                                            batch_size=args.batch_size,\n",
    "                                            one_class_idx=args.one_class_idx,\n",
    "                                            seed=args.seed)\n",
    "    elif args.dataset == 'svhn':\n",
    "        args.num_classes = 10\n",
    "        train_loader, test_loader = load_svhn(data_path, \n",
    "                                            batch_size=args.batch_size,\n",
    "                                            one_class_idx=args.one_class_idx,\n",
    "                                            seed=args.seed)\n",
    "    elif args.dataset == 'cifar100':\n",
    "        args.num_classes = 20\n",
    "        train_loader, test_loader = load_cifar100(data_path, \n",
    "                                                batch_size=args.batch_size,\n",
    "                                                one_class_idx=args.one_class_idx,\n",
    "                                                seed=args.seed)\n",
    "    elif args.dataset == 'imagenet30':\n",
    "        args.num_classes = 30\n",
    "        train_loader, test_loader = load_imagenet(imagenet_path, \n",
    "                                                batch_size=args.batch_size,\n",
    "                                                one_class_idx=args.one_class_idx,\n",
    "                                                seed=args.seed)\n",
    "    elif args.dataset == 'mvtec_ad':\n",
    "        args.num_classes = 15\n",
    "        train_loader, test_loader = load_mvtec_ad(data_path, \n",
    "                                                resize=args.img_size,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                one_class_idx=args.one_class_idx,\n",
    "                                                seed=args.seed)\n",
    "\n",
    "    elif args.dataset == 'visa':\n",
    "        args.num_classes = 12\n",
    "        train_loader, test_loader = load_visa(data_path, \n",
    "                                                resize=args.img_size,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                one_class_idx=args.one_class_idx,\n",
    "                                                seed=args.seed)\n",
    "    \n",
    "    print(\"Start Loading noises\")\n",
    "    train_positives_loader, train_negetives_loader,\\\n",
    "    test_positives_loader, test_negetives_loader = noise_loader(\n",
    "                    args,\n",
    "                    batch_size=args.batch_size,\n",
    "                    one_class_idx=args.one_class_idx,\n",
    "                    resize=args.img_size,\n",
    "                    dataset=args.dataset,\n",
    "                    preprocessing=args.preprocessing, \n",
    "                    k_pairs=args.k_pairs,\n",
    "                    seed=args.seed)\n",
    "    print(\"Loading noises finished!\")\n",
    "\n",
    "    return train_loader, test_loader, train_positives_loader, train_negetives_loader, test_positives_loader, test_negetives_loader\n",
    "\n",
    "\n",
    "def parsing():\n",
    "    parser = argparse.ArgumentParser(description='',\n",
    "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--dataset', default='cifar10', type=str, help='cifar10-cifar100-svhn')\n",
    "    parser.add_argument('--one_class_idx', default=None, type=int, help='select one class index')\n",
    "    parser.add_argument('--config', default=None, help='Config file for reading paths')\n",
    "    parser.add_argument('--batch_size', '-b', type=int, default=1, help='Batch size.')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='1')\n",
    "    parser.add_argument('--img_size', default=32, type=int, help='image size selection')\n",
    "    parser.add_argument('--preprocessing', default='clip', type=str, help='which preprocessing use for noise order')\n",
    "    parser.add_argument('--k_pairs', default=1, type=int, help='Selecting multiple pairs for contrastive loss')\n",
    "    parser.add_argument('--device', type=str, default=\"cuda\", help='cuda or cpu.')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.argv = [\"\", \"--one_class_idx\", \"0\", \"--dataset\", \"cifar10\", \"--batch_size\", \"64\"]\n",
    "\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "root_path = config['root_path']\n",
    "data_path = config['data_path']\n",
    "imagenet_path = config['imagenet_path']\n",
    "args = parsing()\n",
    "args.config = config\n",
    "    \n",
    "\n",
    "train_loader, test_loader, train_positives_loader, train_negetives_loader,\\\n",
    "    test_positives_loader, test_negetives_loader = loading_datasets(args, data_path, imagenet_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, transform = clip.load(\"ViT-L/14\", device=args.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resnet-50 Wide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = wide_resnet50_2(\"IMAGENET1K_V2\")\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(),\n",
    "                                            torchvision.transforms.Resize(256),\n",
    "                                            torchvision.transforms.CenterCrop(224)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (257) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m p_imgs\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m n_imgs\u001b[38;5;241m.\u001b[39mrequires_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m imgs_out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m p_imgs_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(p_imgs)\n\u001b[0;32m     18\u001b[0m n_imgs_out \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(n_imgs)\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\torch\\Lib\\site-packages\\clip\\model.py:341\u001b[0m, in \u001b[0;36mCLIP.encode_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_image\u001b[39m(\u001b[38;5;28mself\u001b[39m, image):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisual\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\torch\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Mohammad\\.conda\\envs\\torch\\Lib\\site-packages\\clip\\model.py:228\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    226\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2, width]\u001b[39;00m\n\u001b[0;32m    227\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_embedding\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice), x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# shape = [*, grid ** 2 + 1, width]\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpositional_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    229\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_pre(x)\n\u001b[0;32m    231\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# NLD -> LND\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (257) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "for (normal, p_data, n_data) in zip(train_loader, train_positives_loader[0], train_negetives_loader[0]):\n",
    "    imgs, labels = normal\n",
    "    p_imgs, p_label = p_data\n",
    "    n_imgs, n_label = n_data\n",
    "    assert torch.equal(torch.squeeze(labels), torch.squeeze(p_label)), f\"The labels of positives images do not match to noraml images, {torch.squeeze(labels)}, {torch.squeeze(p_label)}\"\n",
    "    assert torch.equal(torch.squeeze(labels), torch.squeeze(n_label)), f\"The labels of negatives images do not match to noraml images, {torch.squeeze(labels)}, {torch.squeeze(n_label)}\"\n",
    "    imgs, labels = imgs.to(args.device), labels.to(args.device)\n",
    "    p_imgs = p_imgs.to(args.device)\n",
    "    n_imgs = n_imgs.to(args.device)\n",
    "\n",
    "    imgs.requires_grad = True\n",
    "    p_imgs.requires_grad = True\n",
    "    n_imgs.requires_grad = True\n",
    "\n",
    "    imgs_out = model.encode_image(imgs)\n",
    "    p_imgs_out = model.encode_image(p_imgs)\n",
    "    n_imgs_out = model.encode_image(n_imgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_img = norm_flat_image(imgs_out[0])\n",
    "norm_grad = norm_flat_image(grads)\n",
    "plot_maps(norm_grad, norm_img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
