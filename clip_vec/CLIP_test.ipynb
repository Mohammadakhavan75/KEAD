{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jI_K5ywYI50d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from clip import clip\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
        "cifar10_dataset = CIFAR10(root='./data', train=True, download=True, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "img, label = cifar10_dataset[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([3, 224, 224])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "img.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEvFpFeNJqv_",
        "outputId": "7ae3de2a-5367-4fc3-ded3-f1528e8a3caf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(100.3750, device='cuda:0', dtype=torch.float16, grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# car\n",
        "class_index = 1\n",
        "# class_images = [i for i in range(len(cifar10_dataset)) if cifar10_dataset[i][1] == class_index]\n",
        "\n",
        "all_distance = 0\n",
        "# for image_index in class_images[:10]:\n",
        "for image_index in range(10):\n",
        "\n",
        "    image, _ = cifar10_dataset[image_index]\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    image_features1 = model.encode_image(image)\n",
        "\n",
        "    # Perform rotation (90 degrees in this example)\n",
        "    rotated_image = transforms.functional.to_pil_image(image.cpu().squeeze(0))\n",
        "    rotated_image = transforms.functional.rotate(rotated_image, 90)\n",
        "    rotated_image = transform(rotated_image).unsqueeze(0).to(device)\n",
        "    image_features2 = model.encode_image(rotated_image)\n",
        "\n",
        "    app_distance = torch.sum((image_features2 - image_features1)**2)\n",
        "    #similarity = torch.nn.functional.cosine_similarity(image_features1, image_features2)\n",
        "    all_distance += app_distance\n",
        "all_distance / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRMMFIltK8ni",
        "outputId": "d2a4601d-604d-477b-8fa8-4e3005360da7"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# cat\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m class_index \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m class_images \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(cifar10_dataset)) \u001b[39mif\u001b[39;00m cifar10_dataset[i][\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m class_index]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m all_distance1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_index \u001b[39min\u001b[39;00m class_images[:\u001b[39m10\u001b[39m]:\n",
            "\u001b[1;32m/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb Cell 3\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# cat\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m class_index \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m class_images \u001b[39m=\u001b[39m [i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(cifar10_dataset)) \u001b[39mif\u001b[39;00m cifar10_dataset[i][\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m class_index]\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m all_distance1 \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/clip_vec/CLIP_test.ipynb#W2sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m image_index \u001b[39min\u001b[39;00m class_images[:\u001b[39m10\u001b[39m]:\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/torchvision/datasets/cifar.py:118\u001b[0m, in \u001b[0;36mCIFAR10.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    115\u001b[0m img \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mfromarray(img)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[1;32m    120\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    121\u001b[0m     target \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_transform(target)\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/torchvision/transforms/transforms.py:361\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    353\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m    354\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 361\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/torchvision/transforms/functional.py:490\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    488\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    489\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 490\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49moutput_size, interpolation\u001b[39m=\u001b[39;49mpil_interpolation)\n\u001b[1;32m    492\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39moutput_size, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, antialias\u001b[39m=\u001b[39mantialias)\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39misinstance\u001b[39m(size, \u001b[39mlist\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(size) \u001b[39m==\u001b[39m \u001b[39m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mGot inappropriate size arg: \u001b[39m\u001b[39m{\u001b[39;00msize\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(\u001b[39mtuple\u001b[39;49m(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), interpolation)\n",
            "File \u001b[0;32m~/anaconda3/envs/Torch/lib/python3.9/site-packages/PIL/Image.py:2115\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2107\u001b[0m             \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Image\u001b[39m.\u001b[39mreduce(\u001b[39mself\u001b[39m, factor, box\u001b[39m=\u001b[39mreduce_box)\n\u001b[1;32m   2108\u001b[0m         box \u001b[39m=\u001b[39m (\n\u001b[1;32m   2109\u001b[0m             (box[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2110\u001b[0m             (box[\u001b[39m1\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2111\u001b[0m             (box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m factor_x,\n\u001b[1;32m   2112\u001b[0m             (box[\u001b[39m3\u001b[39m] \u001b[39m-\u001b[39m reduce_box[\u001b[39m1\u001b[39m]) \u001b[39m/\u001b[39m factor_y,\n\u001b[1;32m   2113\u001b[0m         )\n\u001b[0;32m-> 2115\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_new(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mim\u001b[39m.\u001b[39;49mresize(size, resample, box))\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# cat\n",
        "class_index = 3\n",
        "class_images = [i for i in range(len(cifar10_dataset)) if cifar10_dataset[i][1] == class_index]\n",
        "all_distance1 = 0\n",
        "for image_index in class_images[:10]:\n",
        "\n",
        "    image, _ = cifar10_dataset[image_index]\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    image_features1 = model.encode_image(image)\n",
        "\n",
        "    # Perform rotation (90 degrees in this example)\n",
        "    rotated_image = transforms.functional.to_pil_image(image.cpu().squeeze(0))\n",
        "    rotated_image = transforms.functional.rotate(rotated_image, 90)\n",
        "    rotated_image = transform(rotated_image).unsqueeze(0).to(device)\n",
        "    image_features2 = model.encode_image(rotated_image)\n",
        "\n",
        "    app_distance = torch.sum((image_features2 - image_features1)**2)\n",
        "    #similarity = torch.nn.functional.cosine_similarity(image_features1, image_features2)\n",
        "    all_distance1 += app_distance\n",
        "all_distance1 / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkE36m3dM6Z-",
        "outputId": "f48f7ab6-e1a7-4fe4-8e27-c8dd96dc133a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(81.6992, grad_fn=<DivBackward0>)"
            ]
          },
          "execution_count": 76,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# frog\n",
        "class_index = 6\n",
        "class_images = [i for i in range(len(cifar10_dataset)) if cifar10_dataset[i][1] == class_index]\n",
        "all_distance2 = 0\n",
        "for image_index in class_images[:10]:\n",
        "\n",
        "    image, _ = cifar10_dataset[image_index]\n",
        "    image = image.unsqueeze(0).to(device)\n",
        "    image_features1 = model.encode_image(image)\n",
        "\n",
        "    # Perform rotation (90 degrees in this example)\n",
        "    rotated_image = transforms.functional.to_pil_image(image.cpu().squeeze(0))\n",
        "    rotated_image = transforms.functional.rotate(rotated_image, 90)\n",
        "    rotated_image = transform(rotated_image).unsqueeze(0).to(device)\n",
        "    image_features2 = model.encode_image(rotated_image)\n",
        "\n",
        "    app_distance = torch.sum((image_features2 - image_features1)**2)\n",
        "    #similarity = torch.nn.functional.cosine_similarity(image_features1, image_features2)\n",
        "    all_distance2 += app_distance\n",
        "\n",
        "all_distance2 / 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install torch torchvision sentencepiece\n",
        "# !pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from clip import clip\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import PIL\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "\n",
        "class load_np_dataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, imgs_path, targets_path, transform):\n",
        "        self.data = np.load(imgs_path)\n",
        "        self.targets = np.load(targets_path)\n",
        "        self.transform = transform\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img , target = self.data[idx], self.targets[idx]\n",
        "            \n",
        "        img = PIL.Image.fromarray(img)\n",
        "        if transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "def parsing():\n",
        "    parser = argparse.ArgumentParser(description='Tunes a CIFAR Classifier with OE',\n",
        "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
        "    parser.add_argument('--aug', type=str, default='gaussian_noise',\n",
        "                        help='select noise.')\n",
        "    parser.add_argument('--batch_size', '-b', type=int,\n",
        "                        default=64, help='Batch size.')\n",
        "    parser.add_argument('--seed', type=int, default=1,\n",
        "                        help='seed')\n",
        "    parser.add_argument('--num_workers', type=int, \n",
        "                        default=0, help='number of workers')\n",
        "    parser.add_argument('--transform', type=int, \n",
        "                        default=0, help='use transformation dataset')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    return args\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.argv = ['']\n",
        "args = parsing()\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "\n",
        "cifar10_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
        "cifar10_dataset = CIFAR10(root=cifar10_path, train=True, download=True, transform=transform)\n",
        "\n",
        "if args.transform:\n",
        "    cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/{args.aug}.npy'\n",
        "    cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/labels-A.npy'\n",
        "    train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
        "else:\n",
        "    cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/{args.aug}.npy'\n",
        "    cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/labels-C.npy'\n",
        "    train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
        "\n",
        "cifar10_loader = DataLoader(cifar10_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
        "aug_loader = DataLoader(train_aug_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
        "\n",
        "\n",
        "loader = zip(cifar10_loader, aug_loader)\n",
        "diffs = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]\n"
          ]
        }
      ],
      "source": [
        "for i, data in enumerate(tqdm(loader)):\n",
        "    data_normal, data_aug = data\n",
        "    imgs_n, _ = data_normal\n",
        "    imgs_aug, _ = data_aug\n",
        "    # imgs_n, imgs_aug = transform(imgs_n).to(device), transform(imgs_aug).to(device)\n",
        "    imgs_n, imgs_aug = imgs_n.to(device), imgs_aug.to(device)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "imgs_n_features = model.encode_image(imgs_n)\n",
        "imgs_aug_features = model.encode_image(imgs_aug)\n",
        "diffs.append(torch.sum(torch.pow((imgs_n_features - imgs_aug_features), 2), dim=1).detach().cpu().numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(f'./tensors/diffs_{args.aug}.pkl', 'wb') as f:\n",
        "    pickle.dump(diffs, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_10848/2339142470.py:13: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  vals_softmax = torch.nn.functional.softmax(vals).detach().cpu().numpy()\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "augs = os.listdir('./outputs')\n",
        "for aug in augs:\n",
        "    with open(f'./outputs/{aug}', 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        lines.pop(0)\n",
        "    vals = []\n",
        "    for line in lines:\n",
        "        vals.append(float(line.split(':')[-1].split('\\n')[0]))\n",
        "    \n",
        "    vals = torch.tensor(vals)\n",
        "    vals_softmax = torch.nn.functional.softmax(vals).detach().cpu().numpy()\n",
        "    with open(f'./softmax/{aug}', 'w') as file:\n",
        "        for val in vals_softmax:\n",
        "            file.write(str(val)+'\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "augs = os.listdir('./softmax')\n",
        "augs_probs = {}\n",
        "for i in range(10):\n",
        "    augs_probs[i] = {}\n",
        "\n",
        "for aug in augs:\n",
        "    with open(f'./softmax/{aug}', 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "        # augs_probs[i].append({\n",
        "        #     aug.split('.')[0] : float(line.split('\\n')[0])\n",
        "        # })\n",
        "        augs_probs[i][aug.split('.')[0]] = float(line.split('\\n')[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'random_crop': 0.0040205405,\n",
              " 'rot90': 5.038615e-06,\n",
              " 'gaussian_blur': 0.13409017,\n",
              " 'gaussian_noise': 0.03901328,\n",
              " 'motion_blur': 0.03116428,\n",
              " 'jpeg_compression': 0.090828806,\n",
              " 'shot_noise': 0.13746814,\n",
              " 'flip': 0.11583374,\n",
              " 'brightness': 0.11194416,\n",
              " 'snow': 0.3783712,\n",
              " 'fog': 0.4014634,\n",
              " 'glass_blur': 1.2054588e-08,\n",
              " 'elastic_transform': 0.06608733,\n",
              " 'contrast': 0.11341642,\n",
              " 'speckle_noise': 0.13079225,\n",
              " 'color_jitter': 0.031809445,\n",
              " 'saturate': 0.05244525,\n",
              " 'zoom_blur': 0.04244273,\n",
              " 'spatter': 0.5304918,\n",
              " 'rot270': 3.2738772e-06,\n",
              " 'pixelate': 0.030844223,\n",
              " 'impulse_noise': 0.0028079278,\n",
              " 'defocus_blur': 0.13135473}"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "augs_probs[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "augs_sorted = {}\n",
        "for key in augs_probs.keys():\n",
        "    augs_sorted[key] = {k: v for k, v in sorted(augs_probs[key].items(), key=lambda item: item[1])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('./softmax_sorted.pkl', 'wb') as file:\n",
        "    pickle.dump(augs_sorted, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('./softmax_sorted.pkl', 'rb') as file:\n",
        "    oo = pickle.load(file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "glass_blur 1.2054588e-08\n",
            "rot270 3.2738772e-06\n",
            "rot90 5.038615e-06\n",
            "impulse_noise 0.0028079278\n",
            "random_crop 0.0040205405\n",
            "pixelate 0.030844223\n",
            "motion_blur 0.03116428\n",
            "color_jitter 0.031809445\n",
            "gaussian_noise 0.03901328\n",
            "zoom_blur 0.04244273\n",
            "saturate 0.05244525\n",
            "elastic_transform 0.06608733\n",
            "jpeg_compression 0.090828806\n",
            "brightness 0.11194416\n",
            "contrast 0.11341642\n",
            "flip 0.11583374\n",
            "speckle_noise 0.13079225\n",
            "defocus_blur 0.13135473\n",
            "gaussian_blur 0.13409017\n",
            "shot_noise 0.13746814\n",
            "snow 0.3783712\n",
            "fog 0.4014634\n",
            "spatter 0.5304918\n",
            "speckle_noise 0.000108593915\n",
            "shot_noise 0.00057963433\n",
            "color_jitter 0.0011954387\n",
            "spatter 0.002020797\n",
            "impulse_noise 0.002637804\n",
            "snow 0.010860088\n",
            "saturate 0.011297863\n",
            "gaussian_noise 0.01842857\n",
            "random_crop 0.027475407\n",
            "pixelate 0.034814823\n",
            "jpeg_compression 0.04158446\n",
            "fog 0.048989244\n",
            "flip 0.08968437\n",
            "gaussian_blur 0.09388468\n",
            "defocus_blur 0.09428844\n",
            "motion_blur 0.09942751\n",
            "brightness 0.10103441\n",
            "contrast 0.10612949\n",
            "zoom_blur 0.1277044\n",
            "elastic_transform 0.14662114\n",
            "rot90 0.30168277\n",
            "rot270 0.4348907\n",
            "glass_blur 0.9850425\n",
            "rot270 5.903351e-08\n",
            "rot90 7.0757714e-08\n",
            "glass_blur 2.6591994e-07\n",
            "speckle_noise 0.00024857395\n",
            "random_crop 0.0003352232\n",
            "shot_noise 0.00060745154\n",
            "gaussian_noise 0.0007726162\n",
            "zoom_blur 0.0067154155\n",
            "elastic_transform 0.009014096\n",
            "color_jitter 0.011342007\n",
            "motion_blur 0.03369662\n",
            "impulse_noise 0.040622484\n",
            "contrast 0.050921485\n",
            "fog 0.07469968\n",
            "brightness 0.089774154\n",
            "pixelate 0.097259924\n",
            "flip 0.097923554\n",
            "defocus_blur 0.10071317\n",
            "gaussian_blur 0.100969754\n",
            "spatter 0.123084\n",
            "saturate 0.17196137\n",
            "jpeg_compression 0.18347923\n",
            "snow 0.20733032\n",
            "rot270 2.436604e-09\n",
            "rot90 2.7867795e-09\n",
            "glass_blur 8.769109e-08\n",
            "speckle_noise 0.00022282025\n",
            "shot_noise 0.00052776304\n",
            "gaussian_noise 0.0015853056\n",
            "random_crop 0.0015992604\n",
            "color_jitter 0.0035689208\n",
            "saturate 0.017704738\n",
            "zoom_blur 0.019737702\n",
            "spatter 0.03236021\n",
            "jpeg_compression 0.044963516\n",
            "motion_blur 0.06780367\n",
            "fog 0.0693563\n",
            "impulse_noise 0.07018936\n",
            "snow 0.07249605\n",
            "gaussian_blur 0.101117775\n",
            "defocus_blur 0.101354495\n",
            "contrast 0.10530358\n",
            "brightness 0.11129016\n",
            "flip 0.121867836\n",
            "elastic_transform 0.13666593\n",
            "pixelate 0.303113\n",
            "glass_blur 6.21823e-08\n",
            "rot90 1.7863442e-05\n",
            "rot270 1.9744008e-05\n",
            "speckle_noise 8.4573025e-05\n",
            "shot_noise 0.00017956234\n",
            "gaussian_noise 0.00083539734\n",
            "random_crop 0.0013258334\n",
            "elastic_transform 0.015697114\n",
            "zoom_blur 0.025944723\n",
            "snow 0.039108682\n",
            "fog 0.04006202\n",
            "spatter 0.04011592\n",
            "contrast 0.045379046\n",
            "color_jitter 0.05410969\n",
            "pixelate 0.063535936\n",
            "brightness 0.08604037\n",
            "gaussian_blur 0.086448394\n",
            "defocus_blur 0.08758661\n",
            "flip 0.093621984\n",
            "motion_blur 0.1588847\n",
            "saturate 0.18021396\n",
            "jpeg_compression 0.46126673\n",
            "impulse_noise 0.64544934\n",
            "glass_blur 1.0310835e-08\n",
            "rot270 2.997969e-07\n",
            "rot90 3.4828167e-07\n",
            "speckle_noise 0.00018763346\n",
            "shot_noise 0.0002824912\n",
            "gaussian_noise 0.0011780991\n",
            "random_crop 0.0030348632\n",
            "zoom_blur 0.015736269\n",
            "spatter 0.0176628\n",
            "saturate 0.024580594\n",
            "snow 0.026462259\n",
            "jpeg_compression 0.031389594\n",
            "impulse_noise 0.041911982\n",
            "fog 0.044518176\n",
            "contrast 0.08265372\n",
            "elastic_transform 0.08485781\n",
            "brightness 0.09533106\n",
            "defocus_blur 0.097709924\n",
            "gaussian_blur 0.09781548\n",
            "flip 0.10691966\n",
            "motion_blur 0.10750677\n",
            "color_jitter 0.116354115\n",
            "pixelate 0.15725295\n",
            "rot270 1.7169477e-12\n",
            "rot90 1.9032811e-12\n",
            "glass_blur 2.387282e-10\n",
            "speckle_noise 8.913931e-06\n",
            "gaussian_noise 1.1549677e-05\n",
            "shot_noise 1.9223768e-05\n",
            "random_crop 0.00028228623\n",
            "color_jitter 0.0045115384\n",
            "jpeg_compression 0.005990809\n",
            "spatter 0.009092031\n",
            "snow 0.009325483\n",
            "zoom_blur 0.012160046\n",
            "elastic_transform 0.029326074\n",
            "fog 0.06389402\n",
            "motion_blur 0.06469871\n",
            "contrast 0.07659165\n",
            "gaussian_blur 0.092293836\n",
            "defocus_blur 0.093691826\n",
            "impulse_noise 0.102124915\n",
            "brightness 0.11031631\n",
            "flip 0.13863483\n",
            "pixelate 0.25030982\n",
            "saturate 0.42229635\n",
            "glass_blur 3.1267223e-08\n",
            "speckle_noise 2.6199501e-05\n",
            "shot_noise 5.0647883e-05\n",
            "gaussian_noise 0.00014291963\n",
            "spatter 0.0037168148\n",
            "snow 0.005376134\n",
            "impulse_noise 0.009064102\n",
            "zoom_blur 0.018981565\n",
            "elastic_transform 0.020633468\n",
            "saturate 0.024011187\n",
            "jpeg_compression 0.024446242\n",
            "random_crop 0.025016641\n",
            "pixelate 0.025620751\n",
            "fog 0.04053426\n",
            "color_jitter 0.043478355\n",
            "flip 0.06730181\n",
            "contrast 0.07394576\n",
            "motion_blur 0.07834699\n",
            "brightness 0.08933687\n",
            "gaussian_blur 0.090862945\n",
            "defocus_blur 0.091342874\n",
            "rot90 0.20096405\n",
            "rot270 0.2119488\n",
            "glass_blur 1.2054588e-08\n",
            "jpeg_compression 0.003012368\n",
            "pixelate 0.01401162\n",
            "random_crop 0.02704944\n",
            "saturate 0.04052655\n",
            "motion_blur 0.06394495\n",
            "flip 0.06638793\n",
            "elastic_transform 0.076662645\n",
            "impulse_noise 0.08205968\n",
            "contrast 0.090070926\n",
            "brightness 0.097591765\n",
            "defocus_blur 0.10679064\n",
            "gaussian_blur 0.10784975\n",
            "fog 0.13033608\n",
            "zoom_blur 0.16269962\n",
            "rot270 0.2119488\n",
            "spatter 0.23911141\n",
            "snow 0.24050736\n",
            "rot90 0.28340474\n",
            "color_jitter 0.6908262\n",
            "shot_noise 0.8553555\n",
            "speckle_noise 0.8663033\n",
            "gaussian_noise 0.8879382\n",
            "speckle_noise 0.0020172498\n",
            "spatter 0.0023441657\n",
            "impulse_noise 0.0031324695\n",
            "shot_noise 0.0049296464\n",
            "snow 0.010162335\n",
            "glass_blur 0.01495708\n",
            "pixelate 0.023237012\n",
            "color_jitter 0.042804286\n",
            "gaussian_noise 0.050094046\n",
            "saturate 0.054962154\n",
            "fog 0.08614688\n",
            "gaussian_blur 0.09466723\n",
            "defocus_blur 0.09516726\n",
            "flip 0.10182438\n",
            "brightness 0.10734071\n",
            "jpeg_compression 0.11303827\n",
            "rot270 0.14118834\n",
            "rot90 0.21392512\n",
            "contrast 0.25558785\n",
            "motion_blur 0.2945257\n",
            "elastic_transform 0.4144343\n",
            "zoom_blur 0.5678775\n",
            "random_crop 0.9098605\n"
          ]
        }
      ],
      "source": [
        "for key in augs_sorted.keys():\n",
        "    for key2 in augs_sorted[key].keys():\n",
        "        print(key2, augs_sorted[key][key2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "with open('softmax_sorted.csv', 'w', newline='') as csvfile:\n",
        "    filewriter = csv.writer(csvfile, delimiter=',', quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
        "    for key in augs_sorted.keys():\n",
        "        filewriter.writerow([str(key)])\n",
        "        for key2 in augs_sorted[key].keys():\n",
        "            # print(key2, augs_sorted[key][key2])\n",
        "            filewriter.writerow([str(key2), str(augs_sorted[key][key2])])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
