{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision sentencepiece\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "class load_np_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs_path, targets_path, transform):\n",
    "        self.data = np.load(imgs_path)\n",
    "        self.targets = np.load(targets_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img , target = self.data[idx], self.targets[idx]\n",
    "            \n",
    "        img = PIL.Image.fromarray(img)\n",
    "        if transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "def parsing():\n",
    "    parser = argparse.ArgumentParser(description='Tunes a CIFAR Classifier with OE',\n",
    "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--aug', type=str, default='gaussian_noise',\n",
    "                        help='select noise.')\n",
    "    parser.add_argument('--batch_size', '-b', type=int,\n",
    "                        default=64, help='Batch size.')\n",
    "    parser.add_argument('--seed', type=int, default=1,\n",
    "                        help='seed')\n",
    "    parser.add_argument('--num_workers', type=int, \n",
    "                        default=0, help='number of workers')\n",
    "    parser.add_argument('--transform', type=int, \n",
    "                        default=0, help='use transformation dataset')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['', '--aug', 'gaussian_noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = parsing()\n",
    "args.aug = 'spatter'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "cifar10_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
    "cifar10_dataset = CIFAR10(root=cifar10_path, train=True, download=True, transform=transform)\n",
    "\n",
    "if args.transform:\n",
    "    cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/{args.aug}.npy'\n",
    "    cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/labels-A.npy'\n",
    "    train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
    "else:\n",
    "    cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/{args.aug}.npy'\n",
    "    cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/labels-C.npy'\n",
    "    train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
    "\n",
    "cifar10_loader = DataLoader(cifar10_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "aug_loader = DataLoader(train_aug_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, target in cifar10_loader:\n",
    "    break\n",
    "\n",
    "for imgs_noisy, target_noisy in aug_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "img_ = imgs[idx]\n",
    "noisy_img_ = imgs_noisy[idx]\n",
    "fig, axis = plt.subplots(1, 2, figsize=(20, 10))\n",
    "axis[0].imshow(img_.permute(1,2,0).detach().numpy())\n",
    "axis[0].axis('off')\n",
    "axis[1].imshow(noisy_img_.permute(1,2,0).detach().numpy())\n",
    "axis[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./tensors/diffs_gaussian_noise.pkl\", 'rb') as f:\n",
    "    g = pickle.load(f)\n",
    "\n",
    "with open(\"./tensors/diffs_target_gaussian_noise.pkl\", 'rb') as f:\n",
    "    t = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, g = zip(*sorted(zip(t, g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(g[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"class {i}: {np.mean(g[i*5000:(i+1)*5000])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Megan', 'Harriet', 'Henry', 'Beth', 'George']\n",
    "score_list = [9, 6, 5, 6, 10]\n",
    "score, name = zip(*sorted(zip(score_list, name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP CHECK VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = \"/storage/users/makhavan/CSI/exp09/clip_vec/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(dir):\n",
    "    file_path = os.path.join(dir, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        values = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines[1:]:\n",
    "            values.append(float(line.split(':')[1].split('\\n')[0]))\n",
    "            \n",
    "        print(file_name, np.mean(values), \"Min: \", np.min(values), \"Max: \", np.max(values), \"Max diff: \", np.max(values) - np.min(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "cifar10_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
    "cifar10_dataset = CIFAR10(root=cifar10_path, train=True, download=True, transform=transform)\n",
    "cifar10_loader = DataLoader(cifar10_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "diffs = []\n",
    "for i, data in enumerate(cifar10_loader):\n",
    "    imgs_n, targets = data\n",
    "    # imgs_n, imgs_aug = transform(imgs_n).to(device), transform(imgs_aug).to(device)\n",
    "    imgs_n = imgs_n.to(device)\n",
    "    imgs_n_features = model.encode_image(imgs_n)\n",
    "    diffs.extend(torch.mean(imgs_n_features, dim=1).detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./tensors/normal_data.pkl', 'wb') as f:\n",
    "    pickle.dump(diffs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "class_diff = diffs[i*5000:i*5000 + 5000] / np.max(diffs[i*5000:i*5000 + 5000])\n",
    "class_diff = torch.mean(torch.tensor(class_diff), dim=1)\n",
    "class_diff_normalized = (class_diff - torch.mean(class_diff)) / torch.std(class_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idices = [element for i, element in enumerate(class_diff_normalized) if element  < np.percentile(class_diff_normalized, 95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(class_diff), torch.min(class_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './csv_results/report_class_1/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m names\u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./csv_results/report_class_1/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(root):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(filename)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(root \u001b[39m+\u001b[39m filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './csv_results/report_class_1/'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "all_dfs = []\n",
    "names= []\n",
    "root='./csv_results/report_class_1/'\n",
    "for filename in os.listdir(root):\n",
    "    print(filename)\n",
    "    df = pd.read_csv(root + filename)\n",
    "    df.insert(loc=0, column='noise', value=filename.split('.')[0])\n",
    "    all_dfs.append(df)\n",
    "    names.append(filename.split('.')[0])\n",
    "\n",
    "# combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# combined_df.to_csv(root + \"combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision sentencepiece\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "class load_np_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs_path, targets_path, transform):\n",
    "        self.data = np.load(imgs_path)\n",
    "        self.targets = np.load(targets_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img , target = self.data[idx], self.targets[idx]\n",
    "            \n",
    "        img = PIL.Image.fromarray(img)\n",
    "        if transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "def parsing():\n",
    "    parser = argparse.ArgumentParser(description='Tunes a CIFAR Classifier with OE',\n",
    "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--aug', type=str, default='gaussian_noise',\n",
    "                        help='select noise.')\n",
    "    parser.add_argument('--batch_size', '-b', type=int,\n",
    "                        default=64, help='Batch size.')\n",
    "    parser.add_argument('--seed', type=int, default=1,\n",
    "                        help='seed')\n",
    "    parser.add_argument('--num_workers', type=int, \n",
    "                        default=0, help='number of workers')\n",
    "    parser.add_argument('--transform', type=int, \n",
    "                        default=0, help='use transformation dataset')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n",
    "\n",
    "import sys\n",
    "sys.argv = ['']\n",
    "args = parsing()\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "# cifar10_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
    "# cifar10_dataset = CIFAR10(root=cifar10_path, train=True, download=True, transform=transform)\n",
    "\n",
    "# if args.transform:\n",
    "#     cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/{args.aug}.npy'\n",
    "#     cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/labels-A.npy'\n",
    "#     train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
    "# else:\n",
    "#     cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/{args.aug}.npy'\n",
    "#     cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/labels-C.npy'\n",
    "#     train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
    "\n",
    "# cifar10_loader = DataLoader(cifar10_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "# aug_loader = DataLoader(train_aug_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n",
    "\n",
    "# loader = zip(cifar10_loader, aug_loader)\n",
    "# diffs = []\n",
    "# targets_list = []\n",
    "# for i, data in enumerate(tqdm(loader)):\n",
    "#     data_normal, data_aug = data\n",
    "#     imgs_n, targets = data_normal\n",
    "#     imgs_aug, _ = data_aug\n",
    "#     # imgs_n, imgs_aug = transform(imgs_n).to(device), transform(imgs_aug).to(device)\n",
    "#     if len(imgs_n) != len(imgs_aug): # if len imgs_aug was larger than imgs_normal\n",
    "#         imgs_aug = imgs_aug[:len(imgs_n)]\n",
    "#         imgs_n, imgs_aug = imgs_n.to(device), imgs_aug.to(device)\n",
    "#         imgs_n_features = model.encode_image(imgs_n)\n",
    "#         imgs_aug_features = model.encode_image(imgs_aug)\n",
    "#         diffs.extend(torch.sum(torch.pow((imgs_n_features - imgs_aug_features), 2), dim=1).detach().cpu().numpy())\n",
    "#         targets_list.extend(targets.detach().cpu().numpy())\n",
    "#         break\n",
    "\n",
    "#     imgs_n, imgs_aug = imgs_n.to(device), imgs_aug.to(device)\n",
    "#     imgs_n_features = model.encode_image(imgs_n)\n",
    "#     imgs_aug_features = model.encode_image(imgs_aug)\n",
    "#     diffs.extend(torch.sum(torch.pow((imgs_n_features - imgs_aug_features), 2), dim=1).detach().cpu().numpy())\n",
    "#     targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "# diffs = np.asarray(diffs)\n",
    "# targets_list = np.asarray(targets_list)\n",
    "# with open(f'./tensors/diffs_{args.aug}.pkl', 'wb') as f:\n",
    "#     pickle.dump(diffs, f)\n",
    "# with open(f'./tensors/diffs_target_{args.aug}.pkl', 'wb') as f:\n",
    "#     pickle.dump(targets_list, f)\n",
    "\n",
    "\n",
    "# t, g = zip(*sorted(zip(targets_list, diffs)))\n",
    "# for i in range(10):\n",
    "#     print(f\"class {i}: {np.mean(g[i*5000:(i+1)*5000])}\")\n",
    "\n",
    "\n",
    "# vals_softmax=[]\n",
    "# for i in range(10):\n",
    "#     vals = torch.tensor(np.mean(g[i*5000:(i+1)*5000]))\n",
    "#     vals_softmax.append(torch.nn.functional.softmax(vals).detach().cpu().numpy())\n",
    "\n",
    "# with open(f'./softmax/{args.aug}.out', 'w') as file:\n",
    "#     for val in vals_softmax:\n",
    "#         file.write(str(val)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "def load_svhn(svhn_path):\n",
    "\n",
    "    # mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "    # std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "    # transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean, std)])\n",
    "    print('loading SVHN')\n",
    "    train_data = torchvision.datasets.SVHN(root=svhn_path, split=\"train\", transform=transform)\n",
    "\n",
    "    test_data = torchvision.datasets.SVHN(root=svhn_path, split=\"test\", transform=transform)\n",
    "\n",
    "    # train_data.targets = train_data.targets.astype('int64')\n",
    "    # test_data.targets = test_data.targets.astype('int64')\n",
    "    \n",
    "    return train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading SVHN\n"
     ]
    }
   ],
   "source": [
    "svhn_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
    "train_data, test_data = load_svhn(svhn_path)\n",
    "svh_train = DataLoader(train_data, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "svh_test = DataLoader(test_data, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svhn_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/SVHN-R-C/{args.aug}.npy'\n",
    "svhn_train_cor_target_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/SVHN-R-C/labels-C.npy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_aug_dataset = load_np_dataset(svhn_train_cor_img_path, svhn_train_cor_target_path, transform=transform)\n",
    "aug_loader = DataLoader(train_aug_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "loader = zip(svh_train, aug_loader)\n",
    "diffs = []\n",
    "targets_list = []\n",
    "for i, data in enumerate(tqdm(loader)):\n",
    "    data_normal, data_aug = data\n",
    "    imgs_n, targets = data_normal\n",
    "    imgs_aug, _ = data_aug\n",
    "    # imgs_n, imgs_aug = transform(imgs_n).to(device), transform(imgs_aug).to(device)\n",
    "    if len(imgs_n) != len(imgs_aug): # if len imgs_aug was larger than imgs_normal\n",
    "        imgs_aug = imgs_aug[:len(imgs_n)]\n",
    "        imgs_n, imgs_aug = imgs_n.to(device), imgs_aug.to(device)\n",
    "        imgs_n_features = model.encode_image(imgs_n)\n",
    "        imgs_aug_features = model.encode_image(imgs_aug)\n",
    "        diffs.extend(torch.sum(torch.pow((imgs_n_features - imgs_aug_features), 2), dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "        break\n",
    "\n",
    "    imgs_n, imgs_aug = imgs_n.to(device), imgs_aug.to(device)\n",
    "    imgs_n_features = model.encode_image(imgs_n)\n",
    "    imgs_aug_features = model.encode_image(imgs_aug)\n",
    "    diffs.extend(torch.sum(torch.pow((imgs_n_features - imgs_aug_features), 2), dim=1).detach().cpu().numpy())\n",
    "    targets_list.extend(targets.detach().cpu().numpy())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[55.75,\n",
       " 45.0,\n",
       " 42.94,\n",
       " 45.62,\n",
       " 37.16,\n",
       " 49.1,\n",
       " 68.56,\n",
       " 67.75,\n",
       " 25.5,\n",
       " 18.84,\n",
       " 27.53,\n",
       " 49.38,\n",
       " 42.38,\n",
       " 27.55,\n",
       " 53.8,\n",
       " 50.03,\n",
       " 42.03,\n",
       " 33.53,\n",
       " 34.7,\n",
       " 28.47,\n",
       " 52.75,\n",
       " 36.6,\n",
       " 35.16,\n",
       " 21.19,\n",
       " 49.75,\n",
       " 49.22,\n",
       " 70.75,\n",
       " 45.0,\n",
       " 64.4,\n",
       " 57.16,\n",
       " 40.62,\n",
       " 46.62,\n",
       " 40.8,\n",
       " 49.47,\n",
       " 33.3,\n",
       " 31.42,\n",
       " 26.31,\n",
       " 21.14,\n",
       " 77.8,\n",
       " 96.75,\n",
       " 91.8,\n",
       " 36.8,\n",
       " 50.7,\n",
       " 53.62,\n",
       " 31.94,\n",
       " 37.3,\n",
       " 52.8,\n",
       " 34.2,\n",
       " 23.22,\n",
       " 60.88,\n",
       " 52.53,\n",
       " 35.38,\n",
       " 20.02,\n",
       " 42.25,\n",
       " 55.12,\n",
       " 32.2,\n",
       " 31.52,\n",
       " 44.56,\n",
       " 26.98,\n",
       " 35.9,\n",
       " 25.44,\n",
       " 46.56,\n",
       " 45.7,\n",
       " 33.8]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
