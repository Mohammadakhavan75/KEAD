{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision sentencepiece\n",
    "# !pip install git+https://github.com/openai/CLIP.git\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "class load_np_dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, imgs_path, targets_path, transform):\n",
    "        self.data = np.load(imgs_path)\n",
    "        self.targets = np.load(targets_path)\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img , target = self.data[idx], self.targets[idx]\n",
    "            \n",
    "        img = PIL.Image.fromarray(img)\n",
    "        if transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "def parsing():\n",
    "    parser = argparse.ArgumentParser(description='Tunes a CIFAR Classifier with OE',\n",
    "                                    formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n",
    "    parser.add_argument('--aug', type=str, default='gaussian_noise',\n",
    "                        help='select noise.')\n",
    "    parser.add_argument('--batch_size', '-b', type=int,\n",
    "                        default=64, help='Batch size.')\n",
    "    parser.add_argument('--seed', type=int, default=1,\n",
    "                        help='seed')\n",
    "    parser.add_argument('--num_workers', type=int, \n",
    "                        default=0, help='number of workers')\n",
    "    parser.add_argument('--transform', type=int, \n",
    "                        default=0, help='use transformation dataset')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.argv = ['', '--aug', 'gaussian_noise']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "args = parsing()\n",
    "args.aug = 'spatter'\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "cifar10_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
    "cifar10_dataset = CIFAR10(root=cifar10_path, train=True, download=True, transform=transform)\n",
    "\n",
    "if args.transform:\n",
    "    cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/{args.aug}.npy'\n",
    "    cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-A/labels-A.npy'\n",
    "    train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
    "else:\n",
    "    cifar_train_cor_img_path = f'/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/{args.aug}.npy'\n",
    "    cifar_train_cor_target_path = '/storage/users/makhavan/CSI/finals/datasets/generalization_repo_dataset/CIFAR-10-Train-R-C/labels-C.npy'\n",
    "    train_aug_dataset = load_np_dataset(cifar_train_cor_img_path, cifar_train_cor_target_path, transform=transform)\n",
    "\n",
    "cifar10_loader = DataLoader(cifar10_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "aug_loader = DataLoader(train_aug_dataset, shuffle=False, batch_size=args.batch_size, num_workers=args.num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for imgs, target in cifar10_loader:\n",
    "    break\n",
    "\n",
    "for imgs_noisy, target_noisy in aug_loader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 12\n",
    "img_ = imgs[idx]\n",
    "noisy_img_ = imgs_noisy[idx]\n",
    "fig, axis = plt.subplots(1, 2, figsize=(20, 10))\n",
    "axis[0].imshow(img_.permute(1,2,0).detach().numpy())\n",
    "axis[0].axis('off')\n",
    "axis[1].imshow(noisy_img_.permute(1,2,0).detach().numpy())\n",
    "axis[1].axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./tensors/diffs_gaussian_noise.pkl\", 'rb') as f:\n",
    "    g = pickle.load(f)\n",
    "\n",
    "with open(\"./tensors/diffs_target_gaussian_noise.pkl\", 'rb') as f:\n",
    "    t = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, g = zip(*sorted(zip(t, g)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(g[:5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(f\"class {i}: {np.mean(g[i*5000:(i+1)*5000])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['Megan', 'Harriet', 'Henry', 'Beth', 'George']\n",
    "score_list = [9, 6, 5, 6, 10]\n",
    "score, name = zip(*sorted(zip(score_list, name)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP CHECK VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "dir = \"/storage/users/makhavan/CSI/exp09/clip_vec/outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in os.listdir(dir):\n",
    "    file_path = os.path.join(dir, file_name)\n",
    "    with open(file_path, 'r') as f:\n",
    "        values = []\n",
    "        lines = f.readlines()\n",
    "        for line in lines[1:]:\n",
    "            values.append(float(line.split(':')[1].split('\\n')[0]))\n",
    "            \n",
    "        print(file_name, np.mean(values), \"Min: \", np.min(values), \"Max: \", np.max(values), \"Max diff: \", np.max(values) - np.min(values))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from clip import clip\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import PIL\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import pickle\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, transform = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "cifar10_path = '/storage/users/makhavan/CSI/finals/datasets/data/'\n",
    "cifar10_dataset = CIFAR10(root=cifar10_path, train=True, download=True, transform=transform)\n",
    "cifar10_loader = DataLoader(cifar10_dataset, shuffle=False, batch_size=16)\n",
    "\n",
    "diffs = []\n",
    "for i, data in enumerate(cifar10_loader):\n",
    "    imgs_n, targets = data\n",
    "    # imgs_n, imgs_aug = transform(imgs_n).to(device), transform(imgs_aug).to(device)\n",
    "    imgs_n = imgs_n.to(device)\n",
    "    imgs_n_features = model.encode_image(imgs_n)\n",
    "    diffs.extend(torch.mean(imgs_n_features, dim=1).detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'./tensors/normal_data.pkl', 'wb') as f:\n",
    "    pickle.dump(diffs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "class_diff = diffs[i*5000:i*5000 + 5000] / np.max(diffs[i*5000:i*5000 + 5000])\n",
    "class_diff = torch.mean(torch.tensor(class_diff), dim=1)\n",
    "class_diff_normalized = (class_diff - torch.mean(class_diff)) / torch.std(class_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idices = [element for i, element in enumerate(class_diff_normalized) if element  < np.percentile(class_diff_normalized, 95)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(class_diff), torch.min(class_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './csv_results/report_class_1/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb Cell 23\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m names\u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m root\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m./csv_results/report_class_1/\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(root):\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(filename)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B7b22686f73744e616d65223a22564d5f3338227d/storage/users/makhavan/CSI/exp09/new_contrastive/clip_vec/temp.ipynb#X36sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(root \u001b[39m+\u001b[39m filename)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './csv_results/report_class_1/'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "all_dfs = []\n",
    "names= []\n",
    "root='./csv_results/report_class_1/'\n",
    "for filename in os.listdir(root):\n",
    "    print(filename)\n",
    "    df = pd.read_csv(root + filename)\n",
    "    df.insert(loc=0, column='noise', value=filename.split('.')[0])\n",
    "    all_dfs.append(df)\n",
    "    names.append(filename.split('.')[0])\n",
    "\n",
    "# combined_df = pd.concat(all_dfs, ignore_index=True)\n",
    "# combined_df.to_csv(root + \"combined.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
